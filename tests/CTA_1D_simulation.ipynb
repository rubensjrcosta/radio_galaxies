{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n",
    "\"\"\"Session class driving the high level interface API\"\"\"\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import yaml\n",
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from astropy.units import Quantity\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "from astropy.io.fits.verify import VerifyWarning\n",
    "\n",
    "from gammapy.data import (\n",
    "    Observation,\n",
    "    DataStore,\n",
    "    FixedPointingInfo, \n",
    "    PointingMode,\n",
    ")\n",
    "from gammapy.estimators import (\n",
    "    FluxPoints, \n",
    "    SensitivityEstimator,\n",
    "    ExcessMapEstimator,\n",
    "    FluxPointsEstimator,\n",
    "    LightCurveEstimator,\n",
    ")\n",
    "from gammapy.makers import (\n",
    "    FoVBackgroundMaker,\n",
    "    MapDatasetMaker,\n",
    "    ReflectedRegionsBackgroundMaker,\n",
    "    RingBackgroundMaker,\n",
    "    SafeMaskMaker,\n",
    "    SpectrumDatasetMaker,\n",
    ")\n",
    "\n",
    "from gammapy.utils.units import energy_unit_format\n",
    "from gammapy.utils.pbar import progress_bar\n",
    "from gammapy.utils.scripts import make_path, read_yaml\n",
    "\n",
    "from gammapy.datasets import (\n",
    "    Datasets,  \n",
    "    MapDataset, \n",
    "    FluxPointsDataset, \n",
    "    SpectrumDatasetOnOff, \n",
    "    SpectrumDataset,\n",
    ")\n",
    "\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel, \n",
    "    Models,\n",
    "    Model,\n",
    "    DatasetModels, \n",
    "    FoVBackgroundModel, \n",
    "    Models, \n",
    "    SkyModel, \n",
    "    ExpCutoffPowerLawSpectralModel\n",
    ")\n",
    "\n",
    "from gammapy.maps import Map, MapAxis, RegionGeom, WcsGeom\n",
    "from gammapy.stats import WStatCountsStatistic\n",
    "\n",
    "from feupy.utils.datasets import flux_points_dataset_from_table, cut_energy_table_fp, write_datasets, read_datasets\n",
    "from feupy.utils.observation import sensitivity_estimator\n",
    "from feupy.utils.stats import StatisticalUtilityFunctions as stats\n",
    "from feupy.utils.string_handling import name_to_txt\n",
    "from feupy.utils.coordinates import skycoord_to_dict, skycoord_config_to_skycoord\n",
    "from regions import CircleSkyRegion\n",
    "\n",
    "\n",
    "from feupy.cta.irfs import Irfs\n",
    "\n",
    "from feupy.roi import ROI\n",
    "from feupy.target import Target\n",
    "\n",
    "from feupy.catalog.pulsar.atnf import SourceCatalogATNF\n",
    "\n",
    "\n",
    "# from feupy.analysis.config_sml import CTASimulationConfig\n",
    "\n",
    "__all__ = [\"CTASimulation\"]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "class CTASimulation:\n",
    "    \"\"\"Config-driven high level CTASimulation interface.\n",
    "\n",
    "    It is initialized by default with a set of configuration parameters and values declared in\n",
    "    an internal high level interface model, though the user can also provide configuration\n",
    "    parameters passed as a nested dictionary at the moment of instantiation. In that case these\n",
    "    parameters will overwrite the default values of those present in the configuration file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict or `CTASimulationConfig`\n",
    "        Configuration options following `CTASimulationConfig` schema\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.config.set_logging()\n",
    "    \n",
    "#         self._ctao_perf = Irfs\n",
    "#         self._ctao_perf.get_irfs(self.config.observation.irfs_config)\n",
    "        self.fit = Fit()\n",
    "        self.fit_result = None\n",
    "        self.flux_points = None\n",
    "        self.datasets = None\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def config(self):\n",
    "        \"\"\"CTASimulation configuration (`CTASimulationConfig`)\"\"\"\n",
    "        return self._config\n",
    "\n",
    "    @config.setter\n",
    "    def config(self, value):\n",
    "        if isinstance(value, dict):\n",
    "            self._config = CTASimulationConfig(**value)\n",
    "        elif isinstance(value, CTASimulationConfig):\n",
    "            self._config = value\n",
    "        else:\n",
    "            raise TypeError(\"config must be dict or CTASimulationConfig.\")\n",
    "\n",
    "    @property\n",
    "    def models(self):\n",
    "        if not self.datasets:\n",
    "            raise RuntimeError(\"No datasets defined. Impossible to set models.\")\n",
    "        return self.datasets.models\n",
    "\n",
    "    @models.setter\n",
    "    def models(self, models):\n",
    "        self.set_models(models, extend=False)\n",
    "        \n",
    "    def update_config(self, config):\n",
    "        \"\"\"Update the configuration.\"\"\"\n",
    "        self.config = self.config.update(config=config)\n",
    "    \n",
    "        \n",
    "#     def compute_sensitivity():\n",
    "#         log.info(\"computing sensitivity.\")\n",
    "#         spectrum, gamma_min, n_sigma, bkg_syst_fraction, dataset_onoff, sed_type=\"e2dnde\", name=\"sens\"\n",
    "        \n",
    "#         sens, sensitivity_table = sensitivity_estimator(\n",
    "#             spectrum=spectrum,\n",
    "#             gamma_min=gamma_min, \n",
    "#             n_sigma=n_sigma, \n",
    "#             bkg_syst_fraction=bkg_syst_fraction, \n",
    "#             dataset_onoff=dataset_onoff)\n",
    "        \n",
    "#         name = name.replace(\"model\", \"\")\n",
    "#         model_name = f\"model sens {name}\"\n",
    "#         name = f\"sens {name}\"\n",
    "       \n",
    "#         sensitivity_ds = flux_points_dataset_from_table(\n",
    "#             sensitivity_table, \n",
    "#             reference_model=spectrum.copy(),\n",
    "#             sed_type=sed_type,\n",
    "#             name=name,\n",
    "#             model_name=model_name)\n",
    "\n",
    "#         return sens, sensitivity_ds, sensitivity_table\n",
    "    \n",
    "    def create_observation(self):\n",
    "        return self._create_observation(self)\n",
    "    \n",
    "    def _create_observation(self):\n",
    "        \"\"\"Create an observation.\"\"\"\n",
    "        on_region_settings = self.config.datasets.on_region\n",
    "        observation_settings = self.config.observation\n",
    "        \n",
    "        on_lon = on_region_settings.lon\n",
    "        on_lat = on_region_settings.lat\n",
    "        on_center = SkyCoord(on_lon, on_lat, frame=on_region_settings.frame)\n",
    "        print(f\"\\non_center:\\n{on_center}\\n\")\n",
    "        \n",
    "        position_angle = observation_settings.pointing_angle\n",
    "        separation = observation_settings.offset\n",
    "        position = on_center\n",
    "        pointing_position = self._create_pointing_position(position, position_angle, separation)\n",
    "        print(f\"\\npointing_position:\\n{pointing_position}\\n\")\n",
    "        pointing = self._create_pointing(pointing_position)\n",
    "        print(f\"\\npointing:\\n{pointing}\\n\")\n",
    "        livetime = observation_settings.livetime\n",
    "        required_irfs = self.observation_settings.required_irfs\n",
    "        irfs = Irfs.get_irfs(required_irfs)\n",
    "        location = Irfs.get_obs_loc(required_irfs)\n",
    "        \n",
    "        observation = Observation.create(pointing=pointing, livetime=livetime, irfs=irfs, location=location)\n",
    "        print(f\"\\n{observation}\\n\")\n",
    "        return observation\n",
    "        \n",
    "    @staticmethod\n",
    "    def _create_pointing_position(position, position_angle, separation):\n",
    "        \"\"\"Create the pointing position\"\"\"\n",
    "        return position.directional_offset_by(position_angle, separation)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_pointing(pointing_position):\n",
    "        \"\"\"Create the pointing.\"\"\"\n",
    "        return FixedPointingInfo(\n",
    "            mode=PointingMode.POINTING,\n",
    "            fixed_icrs=pointing_position.icrs,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def _create_geometry(self):\n",
    "        \"\"\"Create the geometry.\"\"\"\n",
    "        geom_settings = self.config.datasets.geom\n",
    "        observations_settings = self.config.observation\n",
    "        axes = [self._make_energy_axis(geom_settings.axes.energy)]\n",
    "        center = skycoord_config_to_skycoord(self.config.target.position)\n",
    "        radius = self.config.datasets.on_region_radius\n",
    "        region = self._create_on_region(center, radius)\n",
    "        return RegionGeom.create(region=region, axes=axes)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_energy_axis(config_axis_energy, per_decade=True):\n",
    "        \"\"\"Create the energy axis.\"\"\"\n",
    "        energy_axis = MapAxis.from_energy_bounds(        \n",
    "            energy_min=config_axis_energy.min, \n",
    "            energy_max=config_axis_energy.max, \n",
    "            nbin=config_axis_energy.nbins, \n",
    "            per_decade=per_decade, \n",
    "            name=config_axis_energy.name,\n",
    "            )\n",
    "        return energy_axis\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_on_region(center, radius):\n",
    "        \"\"\"Create the region geometry.\"\"\"\n",
    "        return CircleSkyRegion(\n",
    "            center=center, \n",
    "            radius=radius\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def create_spectrum_dataset_empty(self, name=\"obs-0\"):\n",
    "        log.info(f'***\\nCreating a Spectrum Dataset object with zero filled maps.***')\n",
    "        geom_settings = self.config.datasets.geom\n",
    "        self.geom = self._create_geometry()\n",
    "        log.info(f\"\\n{self.geom}\\n\")\n",
    "        energy_axis_true = self._make_energy_axis(geom_settings.axes.energy_true)\n",
    "        log.info(f\"\\n{energy_axis_true}\\n\")\n",
    "        self.dataset_empty = SpectrumDataset.create(geom=self.geom, energy_axis_true=energy_axis_true, name=name)\n",
    "        log.info(f\"\\ndataset_empty:\\n{self.dataset_empty}\\n\")\n",
    "\n",
    "    def _create_dataset_maker(self):\n",
    "        \"\"\"Create the Spectrum Dataset Maker.\"\"\"\n",
    "        datasets_settings = self.config.datasets\n",
    "            \n",
    "        if datasets_settings.type == \"3d\":\n",
    "            maker = MapDatasetMaker(selection=datasets_settings.selection)\n",
    "            \n",
    "        elif datasets_settings.type == \"1d\":\n",
    "            maker_config = {}\n",
    "            if datasets_settings.containment_correction:\n",
    "                maker_config[\n",
    "                    \"containment_correction\"\n",
    "                ] = datasets_settings.containment_correction\n",
    "\n",
    "            maker_config[\"selection\"] = datasets_settings.map_selection\n",
    "            maker_config[\"use_region_center\"] = datasets_settings.use_region_center\n",
    "            maker = SpectrumDatasetMaker(**maker_config)\n",
    "\n",
    "        return maker\n",
    "    \n",
    "    def _create_safe_mask_maker(self):\n",
    "        \"\"\"Create the Safe Mask Maker.\"\"\"\n",
    "        safe_mask_selection = self.config.datasets.safe_mask.methods\n",
    "        safe_mask_settings = self.config.datasets.safe_mask.parameters\n",
    "\n",
    "        return SafeMaskMaker(methods=safe_mask_selection, **safe_mask_settings)\n",
    "        \n",
    "        \n",
    "    def reduce_data(self, random_state=42):\n",
    "        \"\"\"Make maps and datasets for 3d analysis\"\"\"\n",
    "        log.debug(\"Creating observation.\")\n",
    "        observation = self._create_observation()\n",
    "        target_settings = self.config.observation.target\n",
    "        \n",
    "        log.info(\"Creating reference the dataset, maker and safe maker.\")\n",
    "        self.dataset_empty = self._create_spectrum_dataset_empty()\n",
    "        log.info(f\"\\nDataset Empty:\\n{self.dataset_empty}\\n\")\n",
    "#         self.spectrum_dataset = dataset_empty.copy()\n",
    "        maker = self._create_dataset_maker()\n",
    "        safe_maker = self._create_safe_mask_maker()\n",
    "        log.info(\"Running makers and safing.\")\n",
    "        \n",
    "        dataset = maker.run(self.dataset_empty, observation)\n",
    "        dataset = safe_maker.run(dataset, observation)\n",
    "        print(f\"\\nDataset Reduced:\\n{dataset}\\n\")\n",
    "        log.info(\"Set the model on the dataset, and fake.\")\n",
    "        \n",
    "        models = [Model.from_dict(target_settings.model)]\n",
    "        dataset.models = models\n",
    "        dataset.fake(random_state=random_state)\n",
    "        self.spectrum_dataset = dataset\n",
    "#         print(f\"\\Resulted Dataset:\\n{dataset}\\n\")\n",
    "        print(f\"\\n:\\n{ self.spectrum_dataset }\\n\")\n",
    "\n",
    "    @staticmethod    \n",
    "    def _create_safe_spectrum_dataset_onoff(dataset, acceptance, acceptance_off):\n",
    "    # Spectrum dataset for on-off likelihood fitting.\n",
    "        dataset_onoff = SpectrumDatasetOnOff.from_spectrum_dataset(\n",
    "            dataset=dataset, \n",
    "            acceptance=acceptance, \n",
    "            acceptance_off=acceptance_off,\n",
    "        )\n",
    "        dataset_onoff.fake(\n",
    "            random_state='random-seed', \n",
    "            npred_background=dataset.npred_background()\n",
    "        )\n",
    "        return(dataset_onoff)\n",
    "    \n",
    "    def run_onoff(self): \n",
    "        n_obs = self.config.observation.parameters.n_obs\n",
    "        dataset = self.spectrum_dataset\n",
    "        datasets_onoff_settings = self.config.datasets_onoff\n",
    "        stat_settings = self.config.statistics\n",
    "        sens_settings = self.config.sensitivity\n",
    "\n",
    "        alpha = stat_settings.alpha\n",
    "        acceptance = self.config.datasets_onoff.acceptance \n",
    "        acceptance_off = self.config.datasets_onoff.acceptance_off\n",
    "        dataset_onoff = self._create_safe_spectrum_dataset_onoff(dataset, acceptance, acceptance_off)\n",
    "        \n",
    "        self.spectrum_dataset_onoff = dataset_onoff\n",
    "        _wstat, wstat_dict = self._compute_wstat(dataset_onoff, alpha)\n",
    "        self.config.statistics.wstat = wstat_dict\n",
    "        self.wstat_dict = wstat_dict\n",
    "        self.wstat = _wstat\n",
    "        self.update_config(self.config)\n",
    "        \n",
    "        spectrum = Model.from_dict(self.config.observation.target.model).spectral_model\n",
    "        gamma_min = sens_settings.gamma_min  \n",
    "        n_sigma = sens_settings.n_sigma \n",
    "        bkg_syst_fraction = sens_settings.bkg_syst_fraction\n",
    "        fp_settings = self.config.flux_points\n",
    "        sens, sensitivity_ds, sensitivity_table = self._compute_sensitivity(spectrum, gamma_min, n_sigma, bkg_syst_fraction, dataset_onoff, sed_type=\"e2dnde\", name=fp_settings.source)\n",
    "        self.sens = sens \n",
    "        self.sensitivity_ds = sensitivity_ds\n",
    "        self.sensitivity_table = sensitivity_table\n",
    "        \n",
    "        \n",
    "        datasets = Datasets()\n",
    "\n",
    "        for idx in range(n_obs):\n",
    "            dataset_onoff.fake(\n",
    "                random_state=idx, \n",
    "                npred_background=dataset.npred_background()\n",
    "            )\n",
    "            dataset_fake = dataset_onoff.copy(name=f\"obs-{idx}\")\n",
    "            dataset_fake.meta_table[\"OBS_ID\"] = [idx]\n",
    "            datasets.append(dataset_fake)\n",
    "        self.datasets = datasets\n",
    "        self.table_counts = datasets.info_table()\n",
    "        self.dataset_stacked = datasets.stack_reduce(name=f\"stacked {fp_settings.source}\".replace(\"model\", \"\"))\n",
    "    \n",
    "    @staticmethod    \n",
    "    def _compute_wstat(dataset_onoff, alpha):\n",
    "        log.info(\"computing wstatistics.\")\n",
    "        wstat = stats.compute_wstat(dataset_onoff=dataset_onoff, alpha=alpha)\n",
    "        wstat_dict = wstat.info_dict()\n",
    "        wstat_dict[\"n_on\"] = float(wstat_dict[\"n_on\"])\n",
    "        wstat_dict[\"n_off\"] = float(wstat_dict[\"n_off\"])\n",
    "        wstat_dict[\"background\"] = float(wstat_dict[\"background\"])\n",
    "        wstat_dict[\"excess\"] = float(wstat_dict[\"excess\"])\n",
    "        wstat_dict[\"significance\"] = float(wstat_dict[\"significance\"])\n",
    "        wstat_dict[\"p_value\"] = float(wstat_dict[\"p_value\"])\n",
    "        wstat_dict[\"alpha\"] = float(wstat_dict[\"alpha\"])\n",
    "        wstat_dict[\"mu_sig\"] =float(wstat_dict[\"mu_sig\"])\n",
    "\n",
    "        wstat_dict['error'] = float(wstat.error)\n",
    "        wstat_dict['stat_null'] = float(wstat.stat_null)\n",
    "        wstat_dict['stat_max'] = float(wstat.stat_max)\n",
    "        wstat_dict['ts'] = float(wstat.ts)\n",
    "        print(f\"Number of excess counts: {wstat.n_sig}\")\n",
    "        print(f\"TS: {wstat.ts}\")\n",
    "        print(f\"Significance: {wstat.sqrt_ts}\")\n",
    "        return wstat, wstat_dict\n",
    "    \n",
    "    @staticmethod    \n",
    "    def _compute_sensitivity(spectrum, gamma_min, n_sigma, bkg_syst_fraction, dataset_onoff, sed_type=\"e2dnde\", name=\"sens\"):\n",
    "        log.info(\"computing sensitivity.\")\n",
    "        \n",
    "        sens, sensitivity_table = sensitivity_estimator(\n",
    "            spectrum=spectrum,\n",
    "            gamma_min=gamma_min, \n",
    "            n_sigma=n_sigma, \n",
    "            bkg_syst_fraction=bkg_syst_fraction, \n",
    "            dataset_onoff=dataset_onoff)\n",
    "        name = name.replace(\"model\", \"\")\n",
    "        model_name = f\"model sens {name}\"\n",
    "        name = f\"sens {name}\"\n",
    "       \n",
    "        sensitivity_ds = flux_points_dataset_from_table(\n",
    "            sensitivity_table, \n",
    "            reference_model=spectrum.copy(),\n",
    "            sed_type=sed_type,\n",
    "            name=name,\n",
    "            model_name=model_name)\n",
    "\n",
    "        return sens, sensitivity_ds, sensitivity_table\n",
    "\n",
    "    def fit_model_parameters(self): \n",
    "        datasets = self.datasets\n",
    "        model = Model.from_dict(self.config.observation.target.model)\n",
    "        fitted_parameters, fitted_parameters_dict = self._fit_params(datasets, model)\n",
    "        self.config.statistics.fitted_parameters = fitted_parameters_dict\n",
    "        self.update_config(self.config)\n",
    "        self.fitted_parameters = fitted_parameters\n",
    "        self.fitted_parameters_dict = fitted_parameters_dict\n",
    "        \n",
    "    @staticmethod    \n",
    "    def _fit_params(datasets, model):\n",
    "#         %%time\n",
    "\n",
    "        results = []\n",
    "\n",
    "        fit = Fit()\n",
    "\n",
    "        for dataset in datasets.copy():\n",
    "            dataset.models = model.copy()\n",
    "            result = fit.optimize(dataset)\n",
    "\n",
    "            if result.success:\n",
    "                par_dict = {}\n",
    "                for par in result.parameters.free_parameters:\n",
    "                    par_dict[par.name] = par.quantity\n",
    "                results.append(par_dict)\n",
    "\n",
    "        fitted_params = Table(results).to_pandas()\n",
    "        mean = fitted_params.mean()\n",
    "        uncertainty = fitted_params.std()\n",
    "        fitted_params_dict = {}\n",
    "        for name in list(results[0].keys()):\n",
    "            fitted_params_dict[name] = { \n",
    "                \"mean\": mean[name],\n",
    "                \"uncertainty\": uncertainty[name]\n",
    "            }\n",
    "            print(f\"{name} :\\t {mean[name]:.2e} -+ {uncertainty[name]:.2e}\")\n",
    "    \n",
    "        return fitted_params, fitted_params_dict\n",
    "    \n",
    "    def estimate_flux_points(self):\n",
    "        \"\"\"Estimate flux points for a specific model component.\"\"\"\n",
    "        if not self.datasets:\n",
    "            raise RuntimeError(\n",
    "                \"No datasets defined. Impossible to compute flux points.\"\n",
    "            )\n",
    "\n",
    "        fp_settings = self.config.flux_points\n",
    "        log.info(\"Estimating flux points.\")\n",
    "        energy_edges = self._make_energy_axis(fp_settings.energy).edges\n",
    "        flux_point_estimator = FluxPointsEstimator(\n",
    "            energy_edges=energy_edges,\n",
    "            source=fp_settings.source,\n",
    "            fit=self.fit,\n",
    "            n_jobs=self.config.general.n_jobs,\n",
    "            **fp_settings.parameters,\n",
    "        )\n",
    "\n",
    "        fp = flux_point_estimator.run(datasets=self.datasets)\n",
    "\n",
    "        self.flux_points = FluxPointsDataset(\n",
    "            data=fp, models=self.models[fp_settings.source], name=f\"{fp_settings.source}\".replace(\"model\", \"\")\n",
    "        )\n",
    "        \n",
    "        cols = [\"e_ref\", \"dnde\", \"dnde_ul\", \"dnde_err\", \"sqrt_ts\"]\n",
    "        table = self.flux_points.data.to_table(sed_type=\"dnde\")\n",
    "        log.info(\"\\n{}\".format(table[cols]))\n",
    "\n",
    "\n",
    "    def fit_joint(self):\n",
    "        datasets = self.datasets\n",
    "        model = Model.from_dict(self.config.observation.target.model)\n",
    "\n",
    "        #Compute flux points\n",
    "        datasets.models = [model]\n",
    "\n",
    "        # fit_joint = Fit(backend='sherpa')\n",
    "        fit_joint = Fit()\n",
    "        fit_result_joint = fit_joint.run(datasets=datasets)\n",
    "        print(fit_result_joint)\n",
    "        self.datasets.models = model\n",
    "        self.config.observation.target.model_fitted = model.to_dict()\n",
    "        self.update_config(self.config)\n",
    "\n",
    "        \n",
    "    def set_models(self, models, extend=True):\n",
    "        \"\"\"Set models on datasets.\n",
    "        Adds `FoVBackgroundModel` if not present already\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : `~gammapy.modeling.models.Models` or str\n",
    "            Models object or YAML models string\n",
    "        extend : bool\n",
    "            Extend the exiting models on the datasets or replace them.\n",
    "        \"\"\"\n",
    "        if not self.datasets or len(self.datasets) == 0:\n",
    "            raise RuntimeError(\"Missing datasets\")\n",
    "\n",
    "        log.info(\"Reading model.\")\n",
    "        if isinstance(models, str):\n",
    "            models = Models.from_yaml(models)\n",
    "        elif isinstance(models, Models):\n",
    "            pass\n",
    "        elif isinstance(models, DatasetModels) or isinstance(models, list):\n",
    "            models = Models(models)\n",
    "        else:\n",
    "            raise TypeError(f\"Invalid type: {models!r}\")\n",
    "\n",
    "        if extend:\n",
    "            models.extend(self.datasets.models)\n",
    "\n",
    "        self.datasets.models = models\n",
    "\n",
    "\n",
    "        log.info(models)\n",
    "        \n",
    "        \n",
    "    def read_models(self, path, extend=True):\n",
    "        \"\"\"Read models from YAML file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str\n",
    "            Path to the model file.\n",
    "        extend : bool, optional\n",
    "            Extend the exiting models on the datasets or replace them.\n",
    "            Default is True.\n",
    "        \"\"\"\n",
    "        path = make_path(path)\n",
    "        models = Models.read(path)\n",
    "        self.set_models(models, extend=extend)\n",
    "        log.info(f\"Models loaded from {path}.\")\n",
    "\n",
    "    def write_models(self, overwrite=True, write_covariance=True):\n",
    "        \"\"\"Write models to YAML file.\n",
    "\n",
    "        File name is taken from the configuration file.\n",
    "        \"\"\"\n",
    "        filename_models = self.config.general.models_file\n",
    "        if filename_models is not None:\n",
    "            self.models.write(\n",
    "                filename_models, overwrite=overwrite, write_covariance=write_covariance\n",
    "            )\n",
    "            log.info(f\"Models loaded from {filename_models}.\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Missing models_file in config.general\")\n",
    "\n",
    "    def read_datasets(self):\n",
    "        \"\"\"Read datasets from YAML file.\n",
    "\n",
    "        File names are taken from the configuration file.\n",
    "        \"\"\"\n",
    "        filename = self.config.general.datasets_file\n",
    "        filename_models = self.config.general.models_file\n",
    "        if filename is not None:\n",
    "            self.datasets = Datasets.read(filename)\n",
    "            log.info(f\"Datasets loaded from {filename}.\")\n",
    "            if filename_models is not None:\n",
    "                self.read_models(filename_models, extend=False)\n",
    "        else:\n",
    "            raise RuntimeError(\"Missing datasets_file in config.general\")\n",
    "\n",
    "    def write_datasets(self, overwrite=True, write_covariance=True):\n",
    "        \"\"\"Write datasets to YAML file.\n",
    "\n",
    "        File names are taken from the configuration file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        overwrite : bool, optional\n",
    "            Overwrite existing file. Default is True.\n",
    "        write_covariance : bool, optional\n",
    "            Save covariance or not. Default is True.\n",
    "        \"\"\"\n",
    "        filename = self.config.general.datasets_file\n",
    "        filename_models = self.config.general.models_file\n",
    "        if filename is not None:\n",
    "            self.datasets.write(\n",
    "                filename,\n",
    "                filename_models,\n",
    "                overwrite=overwrite,\n",
    "                write_covariance=write_covariance,\n",
    "            )\n",
    "            log.info(f\"Datasets stored to {filename}.\")\n",
    "            log.info(f\"Datasets stored to {filename_models}.\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Missing datasets_file in config.general\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eAe8JT-6mtC6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "from astropy.units import Quantity\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "from astropy.coordinates import Angle\n",
    "\n",
    "from gammapy.datasets import FluxPointsDataset\n",
    "from gammapy.maps import WcsGeom, MapAxis\n",
    "from gammapy.modeling.models import (\n",
    "    PowerLawSpectralModel,\n",
    "    PointSpatialModel,\n",
    "    SkyModel,\n",
    "    Models,\n",
    "    FoVBackgroundModel,\n",
    "    EBLAbsorptionNormSpectralModel\n",
    ")\n",
    "\n",
    "from feupy.utils.string_handling import name_to_txt\n",
    "from feupy.utils.io import mkdir_sub_directory\n",
    "from feupy.utils.coordinates import skycoord_config_to_skycoord as to_skycoord\n",
    "from feupy.utils.observation import ObservationParameters\n",
    "from feupy.utils.geometry import (\n",
    "    create_energy_axis, \n",
    "    create_pointing, \n",
    "    create_pointing_position, \n",
    "    create_region_geometry,\n",
    "    define_on_region,\n",
    ")\n",
    "\n",
    "from feupy.target import Target\n",
    "from feupy.cta.irfs import Irfs\n",
    "\n",
    "from gammapy.modeling import Fit\n",
    "\n",
    "from gammapy.estimators import FluxPointsEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/born-again/anaconda3/envs/gammapy-1.1/lib/python3.9/site-packages/pydantic/_migration.py:290: UserWarning: `pydantic.utils:deep_update` has been removed. We are importing from `pydantic.v1.utils:deep_update` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
      "  warnings.warn(\n",
      "/home/born-again/anaconda3/envs/gammapy-1.1/lib/python3.9/site-packages/pydantic/_migration.py:290: UserWarning: `pydantic.utils:deep_update` has been removed. We are importing from `pydantic.v1.utils:deep_update` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from feupy.analysis.config import AnalysisConfig\n",
    "from feupy.analysis.core import Analysis\n",
    "\n",
    "from core import load_source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.67, -37.21\n",
      "240.16, -56.69\n",
      "Directory 'Fornax_A' created\n",
      "Directory 'Fornax_A/datasets' created\n",
      "Directory 'Fornax_A/figures' created\n",
      "Directory 'Fornax_A/tables' created\n",
      "Directory 'Fornax_A/data' created\n"
     ]
    }
   ],
   "source": [
    "config = AnalysisConfig()\n",
    "# source_name = 'Cen A'\n",
    "source_name = 'Fornax A'\n",
    "\n",
    "position = SkyCoord.from_name(source_name)\n",
    "print(f'{position.ra.deg:.2f}, {position.dec.deg:.2f}')\n",
    "print(f'{position.galactic.l.deg:.2f}, {position.galactic.b.deg:.2f}')\n",
    "outdir = f\"./{name_to_txt(source_name)}\"\n",
    "outdir_path = mkdir_sub_directory(outdir)\n",
    "\n",
    "datasets_path = mkdir_sub_directory(outdir, 'datasets')[1]\n",
    "figures_path = mkdir_sub_directory(outdir, 'figures')[1]\n",
    "tables_path = mkdir_sub_directory(outdir, 'tables')[1]\n",
    "data_path = mkdir_sub_directory(outdir, 'data')[1]\n",
    "\n",
    "config.general.outdir = outdir\n",
    "config.general.datasets_file = f'{datasets_path}/datasets.yaml'\n",
    "config.general.models_file = f'{datasets_path}/models.yaml'\n",
    "# config.general.data_path = data_path\n",
    "# config.general.figures_path = figures_path\n",
    "# config.general.tables_path = tables_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Fornax_A/datasets/models.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m datasets_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mgeneral\u001b[38;5;241m.\u001b[39moutdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/datasets\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m sky_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_source_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m display(sky_model\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mto_table())\n",
      "File \u001b[0;32m~/Documents/GitHub/radio_galaxies/core.py:19\u001b[0m, in \u001b[0;36mload_source_model\u001b[0;34m(datasets_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_source_model\u001b[39m(datasets_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load the `SkyModel` already prepared for the CTA simulation\"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatasets_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/models.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect(name_substring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-fit\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/anaconda3/envs/gammapy-1.1/lib/python3.9/site-packages/gammapy/modeling/models/core.py:416\u001b[0m, in \u001b[0;36mDatasetModels.read\u001b[0;34m(cls, filename)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename):\n\u001b[1;32m    415\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read from YAML file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m     yaml_str \u001b[38;5;241m=\u001b[39m \u001b[43mmake_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     path, filename \u001b[38;5;241m=\u001b[39m split(filename)\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_yaml(yaml_str, path\u001b[38;5;241m=\u001b[39mpath)\n",
      "File \u001b[0;32m~/anaconda3/envs/gammapy-1.1/lib/python3.9/pathlib.py:1266\u001b[0m, in \u001b[0;36mPath.read_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;124;03m    Open the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/gammapy-1.1/lib/python3.9/pathlib.py:1252\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, buffering\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1247\u001b[0m          errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;124;03m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;124;03m    the built-in open() function does.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gammapy-1.1/lib/python3.9/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath._opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Fornax_A/datasets/models.yaml'"
     ]
    }
   ],
   "source": [
    "datasets_path = f'{config.general.outdir}/datasets'\n",
    "sky_model = load_source_model(datasets_path)\n",
    "display(sky_model.parameters.to_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_intrinsic = sky_model.spectral_model\n",
    "print(model_intrinsic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = Target(\n",
    "    name=source_name, \n",
    "    pos_ra=position.ra, \n",
    "    pos_dec=position.dec,\n",
    "    spectral_model=model_intrinsic,\n",
    "#     redshift = 0.,\n",
    "#     ebl_model_name= \"dominguez\",\n",
    "\n",
    ")\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = target.sky_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livetime = 50 * u.h\n",
    "offset = 0.5*u.deg\n",
    "pointing_angle = 0*u.deg\n",
    "\n",
    "irfs_opt = ['South', 'AverageAz', '20deg', '0.5h']\n",
    "obs_param = ObservationParameters(livetime, offset, pointing_angle, irfs_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.observation  = obs_param.dict\n",
    "config.observation.target = target.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_region_radius = Angle(\"0.11 deg\")\n",
    "\n",
    "config.datasets.on_region = target.dict['position']\n",
    "config.datasets.on_region.radius  = on_region_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_edges_min = 1.0e-01*u.TeV\n",
    "e_edges_max = 3.2e+01*u.TeV\n",
    "nbin_edges = 12\n",
    "config.datasets.geom.axes.energy.min = e_edges_min\n",
    "config.datasets.geom.axes.energy.max = e_edges_max\n",
    "config.datasets.geom.axes.energy.nbins = nbin_edges\n",
    "config.datasets.geom.axes.energy.name = 'energy'\n",
    "energy_settings = config.datasets.geom.axes.energy\n",
    "\n",
    "e_edges_min = 3.2e-02*u.TeV\n",
    "e_edges_max = 1.0e+02*u.TeV\n",
    "nbin_edges = 15\n",
    "config.datasets.geom.axes.energy_true.min =  e_edges_min\n",
    "config.datasets.geom.axes.energy_true.max = e_edges_max\n",
    "config.datasets.geom.axes.energy_true.nbins = nbin_edges\n",
    "config.datasets.geom.axes.energy_true.name = 'energy_true'\n",
    "energy_true_settings = config.datasets.geom.axes.energy_true\n",
    "\n",
    "energy_axis = create_energy_axis(\n",
    "    energy_settings.min, \n",
    "    energy_settings.max, \n",
    "    energy_settings.nbins, \n",
    "    per_decade=True, \n",
    "    name=energy_settings.name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the Fermi/LAT model\n",
    "# import astropy.units as u\n",
    "\n",
    "# energy_bounds=[10 * u.GeV, 2 *u.TeV]\n",
    "# model_intrinsic.plot(energy_bounds=energy_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.write(path=f'{outdir}/config.yaml', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redshift = target.redshift\n",
    "\n",
    "# dominguez = EBLAbsorptionNormSpectralModel.read_builtin(\"dominguez\", redshift=redshift)\n",
    "# franceschini = EBLAbsorptionNormSpectralModel.read_builtin(\"franceschini\", redshift=redshift)\n",
    "# finke = EBLAbsorptionNormSpectralModel.read_builtin(\"finke\", redshift=redshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start customised plot\n",
    "# energy_bounds = [0.08, 3] * u.TeV\n",
    "# ax = plt.gca()\n",
    "# opts = dict(energy_bounds=energy_bounds, ax=ax)\n",
    "# franceschini.plot(label='Franceschini 2008', **opts)\n",
    "# finke.plot(label='Finke 2010', **opts)\n",
    "# dominguez.plot(label='Dominguez 2011', **opts)\n",
    "\n",
    "# # tune plot\n",
    "# ax.set_ylabel(r'Absorption coefficient [$\\exp{(-au(E))}$]')\n",
    "# ax.set_xlim([0.008, 30])  # we get ride of units\n",
    "# ax.set_ylim([1.e-1, 2.])\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_title('EBL models (z=' + str(redshift) + ')')\n",
    "# plt.legend(loc='best') # legend\n",
    "\n",
    "# # show plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redshift = 0.116\n",
    "# ebl = EBLAbsorptionNormSpectralModel.read_builtin(\"dominguez\", redshift=redshift)\n",
    "\n",
    "# spectral_model = ebl * model_intrinsic\n",
    "# spatial_model = sky_model.spatial_model\n",
    "# model_simu = SkyModel(spectral_model=spectral_model, \n",
    "#                       spatial_model=spatial_model,\n",
    "#                       name=\"model-simu\")\n",
    "# display(model_simu.parameters.to_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irfs_groups =[\n",
    "#     ['South', 'South-SSTSubArray','South-MSTSubArray','North','North-MSTSubArray', 'North-LSTSubArray'], \n",
    "#     ['AverageAz', 'SouthAz', 'NorthAz'], \n",
    "#     ['20deg','40deg','60deg'], \n",
    "#     ['0.5h', '5h', '50h']\n",
    "# ]\n",
    "# IRFS_OPTS, IRFS, IRFS_LABELS, LOCATION = Irfs.get_irf_groups(irfs_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_min = 10\n",
    "n_sigma = 5 \n",
    "bkg_syst_fraction = 0.10\n",
    "\n",
    "containment = 0.68\n",
    "\n",
    "acceptance = 1\n",
    "acceptance_off = 20\n",
    "config.onoff.acceptance = acceptance\n",
    "config.onoff.acceptance_off = acceptance_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_selection = [\"edisp\", \"background\", \"exposure\"]\n",
    "methods = [\"bkg-peak\"]\n",
    "parameters = {'aeff_percent': 10}\n",
    "containment_correction = False\n",
    "use_region_center = False  \n",
    "\n",
    "# map_selection = [\"background\", \"edisp\", \"exposure\"]\n",
    "# methods = [\"edisp-bias\"]\n",
    "# parameters = {'bias_percent': 10}\n",
    "\n",
    "config.datasets.map_selection = map_selection\n",
    "config.datasets.safe_mask.parameters = parameters\n",
    "config.datasets.safe_mask.methods = methods\n",
    "config.datasets.containment_correction = containment_correction\n",
    "config.datasets.use_region_center = use_region_center\n",
    "config.statistics.n_obs = n_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.flux_points.source = target.name\n",
    "config.flux_points.energy.nbins = config.datasets.geom.axes.energy.nbins\n",
    "config.flux_points.energy.max = config.datasets.geom.axes.energy.max \n",
    "config.flux_points.energy.min = config.datasets.geom.axes.energy.min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = Analysis(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "observation = analysis.create_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dataset = analysis.create_reference_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spectrum_dataset = analysis.simulate_spectrum(observation, reference_dataset, model.copy(name='source'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_on_off = analysis.make_on_off(spectrum_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax_spectrum, ax_residuals = dataset_on_off.plot_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_on_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_datasets = analysis.run_on_off(spectrum_dataset, dataset_on_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_datasets.models = [model.copy('source')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simulated_datasets.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "fit = Fit()\n",
    "for dataset in simulated_datasets.copy():\n",
    "    # Do the 3D fit\n",
    "    model_fit = model.copy(name=\"model-simu\")\n",
    "    spectral_model = model_fit.spectral_model\n",
    "#     spectral_model.lambda_.min = 0\n",
    "    dataset.models = [model_fit]\n",
    "\n",
    "    fit_result = fit.run([dataset])\n",
    "\n",
    "    if fit_result.success:\n",
    "        _par_dict = {}\n",
    "        _par_dict[\"irfs_opt\"] = f'{irfs_opt}'\n",
    "        _par_dict[\"success\"] = fit_result.success\n",
    "\n",
    "        for par in fit_result.parameters.free_parameters:\n",
    "            _par_dict[par.name] = par.quantity\n",
    "\n",
    "        sp_model = fit_result.models[\"model-simu\"].spectral_model\n",
    "#         Ec = 1./sp_model.lambda_.quantity\n",
    "#         err = Ec**2 * sp_model.lambda_.error / u.TeV\n",
    "#         _par_dict[\"ecut\"] = Ec\n",
    "#         _par_dict[\"ecut_err\"] = err\n",
    "        results.append(_par_dict)\n",
    "\n",
    "\n",
    "# results_all.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_datasets.models.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points_settings = analysis.config.flux_points\n",
    "energy_settings = flux_points_settings.energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_edges = MapAxis.from_energy_bounds(\n",
    "    energy_settings.min,\n",
    "    energy_settings.max, \n",
    "    nbin=energy_settings.nbins).edges\n",
    "\n",
    "fpe = FluxPointsEstimator(\n",
    "    energy_edges=energy_edges, \n",
    "    source='source', \n",
    "    selection_optional=\"all\"\n",
    ")\n",
    "flux_points = fpe.run(datasets=simulated_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cta = FluxPointsDataset(models=model_fit, data=flux_points, name=Irfs.get_irfs_label(irfs_opt))\n",
    "dataset_cta.plot_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cta.plot_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 100\n",
    "config.observations.parameters.n_obs = n_obs\n",
    "\n",
    "offset = 0.5*u.deg \n",
    "config.observations.parameters.offset = offset\n",
    "\n",
    "config.observations.on_region_radius = on_region_radius\n",
    "\n",
    "pointing_angle = 0*u.deg\n",
    "config.observations.pointing_angle = pointing_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointing_position = create_pointing_position(position, pointing_angle, offset)\n",
    "pointing = create_pointing(pointing_position)\n",
    "print(f\"{pointing}\\n\")\n",
    "\n",
    "on_region = define_on_region(center=position, radius=on_region_radius)\n",
    "print(f\"{on_region}\\n\")\n",
    "\n",
    "e_edges_min = 1.0e-01*u.TeV\n",
    "e_edges_max = 3.2e+01*u.TeV\n",
    "nbin_edges = 12\n",
    "config.datasets.geom.axes.energy.min = e_edges_min\n",
    "config.datasets.geom.axes.energy.max = e_edges_max\n",
    "config.datasets.geom.axes.energy.nbins = nbin_edges\n",
    "config.datasets.geom.axes.energy.name = 'energy'\n",
    "energy_settings = config.datasets.geom.axes.energy\n",
    "\n",
    "e_edges_min = 3.2e-02*u.TeV\n",
    "e_edges_max = 1.0e+02*u.TeV\n",
    "nbin_edges = 15\n",
    "config.datasets.geom.axes.energy_true.min =  e_edges_min\n",
    "config.datasets.geom.axes.energy_true.max = e_edges_max\n",
    "config.datasets.geom.axes.energy_true.nbins = nbin_edges\n",
    "config.datasets.geom.axes.energy_true.name = 'energy_true'\n",
    "energy_true_settings = config.datasets.geom.axes.energy_true\n",
    "\n",
    "energy_axis = create_energy_axis(\n",
    "    energy_settings.min, \n",
    "    energy_settings.max, \n",
    "    energy_settings.nbins, \n",
    "    per_decade=True, \n",
    "    name=energy_settings.name\n",
    ")\n",
    "\n",
    "geom = create_region_geometry(on_region, axes=[energy_axis])\n",
    "print(geom)\n",
    "print(energy_axis)\n",
    "\n",
    "energy_axis_true = create_energy_axis(\n",
    "    energy_true_settings.min, \n",
    "    energy_true_settings.max, \n",
    "    energy_true_settings.nbins, \n",
    "    per_decade=True, \n",
    "    name=energy_true_settings.name\n",
    ")\n",
    "\n",
    "\n",
    "empty_dataset = SpectrumDataset.create(geom=geom, energy_axis_true=energy_axis_true, name='empty_dataset')\n",
    "# analysis.datasets = Datasets(empty_dataset)\n",
    "print(empty_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_maker = SpectrumDatasetMaker(selection=selection)\n",
    "sensitivity_estimator = SensitivityEstimator(\n",
    "    gamma_min=gamma_min, n_sigma=n_sigma, bkg_syst_fraction=bkg_syst_fraction\n",
    ")\n",
    "sensitivity_estimator1 = SensitivityEstimator(\n",
    "    gamma_min=gamma_min, n_sigma=n_sigma, bkg_syst_fraction=bkg_syst_fraction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livetime = 50 * u.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Irfs.IRFS_OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irfs_config =[\n",
    "    ['South', 'South-SSTSubArray','South-MSTSubArray','North','North-MSTSubArray', 'North-LSTSubArray'], \n",
    "    ['AverageAz'], \n",
    "    ['20deg','40deg','60deg'], \n",
    "    ['50h']\n",
    "]\n",
    "irfs_opts, irfss, irfs_labels, locations = Irfs.get_irf_groups(irfs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Datasets()\n",
    "for i, (irfs_opt, irfs, irfs_label, location) in enumerate(zip(irfs_opts, irfss, irfs_labels, locations)):\n",
    "    obs = Observation.create(\n",
    "        pointing=pointing, irfs=irfs, livetime=livetime, location=location\n",
    "    )\n",
    "    print(obs)\n",
    "    obs_label = get_obs_label(irfs_opt, offset, on_region_radius, livetime)\n",
    "    print(obs_label)\n",
    "    dataset = spectrum_maker.run(empty_dataset, obs)\n",
    "    print(dataset)\n",
    "\n",
    "    # correct exposure\n",
    "    print('correct exposure')\n",
    "    dataset.exposure *= containment\n",
    "    print(dataset)\n",
    "\n",
    "    # correct background estimation\n",
    "    print('correct background estimation')\n",
    "    on_radii = obs.psf.containment_radius(\n",
    "        energy_true=energy_axis.center, \n",
    "        offset=offset, \n",
    "        fraction=containment\n",
    "    )\n",
    "    factor = (1 - np.cos(on_radii)) / (1 - np.cos(geom.region.radius))\n",
    "    dataset.background *= factor.value.reshape((-1, 1, 1))\n",
    "    print(dataset)\n",
    "\n",
    "    print('create SpectrumDatasetOnOff')\n",
    "    dataset_on_off = SpectrumDatasetOnOff.from_spectrum_dataset(\n",
    "        dataset=dataset, acceptance=acceptance, acceptance_off=acceptance_off\n",
    "    )\n",
    "    print(dataset_on_off)\n",
    "\n",
    "    sensitivity_table = sensitivity_estimator.run(dataset_on_off)\n",
    "    sensitivity_table.meta['source'] = source_name\n",
    "    sensitivity_table.meta[\"offset\"] = offset.to_string()\n",
    "    sensitivity_table.meta[\"on_region_radius\"] = f'{on_region_radius.deg}deg'\n",
    "    sensitivity_table.meta[\"livetime\"] = livetime.to_string()    \n",
    "    sensitivity_table.meta[\"site\"] = irfs_opt[0]\n",
    "    sensitivity_table.meta[\"azimuth-averaged \"] = irfs_opt[1]\n",
    "    sensitivity_table.meta[\"zenith-angle\"] = u.Quantity(irfs_opt[2]).to_string()\n",
    "    sensitivity_table.meta[\"obs_time\"] = u.Quantity(irfs_opt[3]).to_string()\n",
    "    sensitivity_table.meta['irfs_label'] = irfs_label\n",
    "    sensitivity_table.meta['irfs_config'] = irfs_opt\n",
    "    sensitivity_table[\"on_radii\"] = on_radii\n",
    "    sensitivity_table[\"on_radii\"].format = '.3e'\n",
    "    label_table = f'sens-{irfs_label_txt(irfs_opt)}'\n",
    "    label_dataset = f'sens {obs_label}'\n",
    "    sensitivity_dataset = flux_points_dataset_from_table(sensitivity_table, name=label_dataset)\n",
    "    datasets.append(sensitivity_dataset)\n",
    "    print(sensitivity_table)\n",
    "    print(dataset_on_off)\n",
    "\n",
    "    dataset_on_off1 = dataset_on_off.to_image()\n",
    "\n",
    "    sensitivity_table1 = sensitivity_estimator1.run(dataset_on_off1)\n",
    "    print(sensitivity_table1)\n",
    "\n",
    "    # To get the integral flux, we convert to a `FluxPoints` object that does the conversion\n",
    "    # internally\n",
    "    flux_points = FluxPoints.from_table(\n",
    "        sensitivity_table1, sed_type=\"e2dnde\", reference_model=sensitivity_estimator1.spectrum\n",
    "    )\n",
    "    int_sens = np.squeeze(flux_points.flux.quantity)\n",
    "    print(\n",
    "        f\"Integral sensitivity in {livetime:.2f} above {e_edges_min:.2e} \"\n",
    "        f\"is {int_sens:.2e}\"\n",
    "    )\n",
    "    sensitivity_table.meta['int_sens'] = int_sens.to_string()\n",
    "    write_tables_csv(\n",
    "        sensitivity_table, sensitivity_path, label_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.write(f'{datasets_path}/sensitivity_datasets.yaml', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.extend(analysis.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.datasets = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.write_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
